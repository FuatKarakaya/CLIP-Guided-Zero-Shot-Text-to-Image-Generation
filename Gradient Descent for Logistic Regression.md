# Gradient Descent for Logistic Regression (Step by Step)

> Your notes derive **how to update the weights** to minimize cross-entropy loss. Let's break it into digestible pieces.

---

## ðŸŽ¯ The Big Picture

We want to find the best weights **w** so our model predicts well. The method:

```
w_new = w_old - Î± Ã— (âˆ‚Loss/âˆ‚w)
```

Your notes derive that gradient (âˆ‚Loss/âˆ‚w). It's done in **3 stages**:

1. **Sigmoid derivative** (the first image)
2. **Gradient for bias wâ‚€** (second image, left side)
3. **Gradient for weights wâ±¼** (third image)

---

## ðŸ§® Stage 1: Derivative of Sigmoid

### The Sigmoid Function:

```
g(s) = 1 / (1 + e^(-s))
```

This is your **Å·** - the predicted probability.

### Goal: Find dg/ds

Your notes show:

```
dg     e^(-s)         1 + e^(-s)        1
â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ds   (1+e^(-s))Â²   (1+e^(-s))Â²    (1+e^(-s))Â²
```

Let me make this clearer:

```
     1 + e^(-s) - 1        1           1
   = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â²
      (1+e^(-s))Â²       1+e^(-s)    (1+e^(-s))

   = g(s) - g(s)Â²

   = g(s) Ã— (1 - g(s))
```

### âœ¨ The Beautiful Result:

```
dg/ds = g(s) Ã— [1 - g(s)]
```

Or in ML notation:

```
dÅ·/ds = Å· Ã— (1 - Å·)
```

**This is why sigmoid is popular** - its derivative has a super simple form!

---

## ðŸ“ Quick Reference: What Is "s"?

In logistic regression:

```
s = wâ‚€ + wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™
```

Or in vector form: **s = wÂ·x + b**

Then: **Å· = sigmoid(s)**

---

## ðŸ”— Stage 2: Gradient of Cross-Entropy w.r.t. Bias (wâ‚€)

### Starting Point - Cross Entropy Loss:

```
L = -Î£áµ¢ [yâ± log(Å·â±) + (1-yâ±) log(1-Å·â±)]
```

### Step 2.1: Derivative w.r.t. Å·

```
âˆ‚L       yâ±    (1-yâ±)
â”€â”€â”€â”€ = - â”€â”€ + â”€â”€â”€â”€â”€â”€â”€â”€
âˆ‚Å·â±      Å·â±    1-Å·â±
```

Combine into single fraction:

```
     -yâ±(1-Å·â±) + (1-yâ±)Å·â±      -yâ± + yâ±Å·â± + Å·â± - yâ±Å·â±      Å·â± - yâ±
   = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Å·â±(1-Å·â±)                   Å·â±(1-Å·â±)               Å·â±(1-Å·â±)
```

### Step 2.2: Chain Rule

We need âˆ‚L/âˆ‚wâ‚€. Use the chain rule:

```
âˆ‚L      âˆ‚L     âˆ‚Å·â±    âˆ‚s
â”€â”€â”€â”€ = â”€â”€â”€â”€ Ã— â”€â”€â”€â”€ Ã— â”€â”€â”€â”€
âˆ‚wâ‚€    âˆ‚Å·â±     âˆ‚s    âˆ‚wâ‚€
```

We know:
- âˆ‚L/âˆ‚Å·â± = (Å·â± - yâ±) / [Å·â±(1-Å·â±)]
- âˆ‚Å·â±/âˆ‚s = Å·â±(1-Å·â±)  â† from Stage 1!
- âˆ‚s/âˆ‚wâ‚€ = 1  (since s = wâ‚€ + wâ‚xâ‚ + ...)

### Step 2.3: Multiply Them Together

```
âˆ‚L      (Å·â± - yâ±)
â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã— Å·â±(1-Å·â±) Ã— 1
âˆ‚wâ‚€     Å·â±(1-Å·â±)
```

**The Å·â±(1-Å·â±) terms cancel!**

```
âˆ‚L
â”€â”€â”€â”€ = Å·â± - yâ±
âˆ‚wâ‚€
```

For all data points:

```
âˆ‚L
â”€â”€â”€â”€ = Î£áµ¢ (Å·â± - yâ±)
âˆ‚wâ‚€
```

### ðŸŽ‰ Result for Bias Update:

```
wâ‚€_new = wâ‚€_old - Î± Ã— Î£áµ¢(Å·â± - yâ±)
```

---

## ðŸ“Š Stage 3: Gradient w.r.t. Weights (wâ±¼)

Same logic, but now âˆ‚s/âˆ‚wâ±¼ = xâ±¼ (not 1):

```
âˆ‚L      âˆ‚L     âˆ‚Å·â±    âˆ‚s
â”€â”€â”€â”€ = â”€â”€â”€â”€ Ã— â”€â”€â”€â”€ Ã— â”€â”€â”€â”€
âˆ‚wâ±¼    âˆ‚Å·â±     âˆ‚s    âˆ‚wâ±¼

     (Å·â± - yâ±)
   = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã— Å·â±(1-Å·â±) Ã— xâ±¼â±
      Å·â±(1-Å·â±)

   = (Å·â± - yâ±) Ã— xâ±¼â±
```

For all data points:

```
âˆ‚L
â”€â”€â”€â”€ = Î£áµ¢ (Å·â± - yâ±) Ã— xâ±¼â±
âˆ‚wâ±¼
```

### ðŸŽ‰ Result for Weight Update:

```
wâ±¼_new = wâ±¼_old - Î± Ã— Î£áµ¢(Å·â± - yâ±)xâ±¼â±
```

---

## ðŸŽ¨ Summary: The Final Update Rules

| Parameter | Gradient | Update Rule |
|-----------|----------|-------------|
| **Bias (wâ‚€)** | Î£áµ¢(Å·â± - yâ±) | wâ‚€ = wâ‚€ - Î± Ã— Î£(Å· - y) |
| **Weights (wâ±¼)** | Î£áµ¢(Å·â± - yâ±)xâ±¼â± | wâ±¼ = wâ±¼ - Î± Ã— Î£(Å· - y)xâ±¼ |

### In Vector Form:

```
w = w - Î± Ã— Xáµ€(Å· - y)
b = b - Î± Ã— Î£(Å· - y)
```

---

## ðŸ’¡ Why This Is Beautiful

1. **Simple result**: Despite all the calculus, the gradient is just **(prediction - truth) Ã— input**

2. **Intuitive meaning**:
   - If Å· > y (predicted too high) â†’ gradient is positive â†’ weights decrease
   - If Å· < y (predicted too low) â†’ gradient is negative â†’ weights increase
   - Bigger error â†’ bigger update

3. **The sigmoid derivative canceled perfectly** - this isn't a coincidence! Cross-entropy + sigmoid are mathematically "made for each other"

---

## ðŸ”¢ Concrete Example

Say for one data point:
- **x** = [1, 2, 3]
- **y** = 1 (true label)
- **Å·** = 0.7 (predicted prob)
- **Î±** = 0.1 (learning rate)

Error = Å· - y = 0.7 - 1 = **-0.3**

Updates:
```
Î”wâ‚€ = -0.1 Ã— (-0.3) Ã— 1 = +0.03
Î”wâ‚ = -0.1 Ã— (-0.3) Ã— 1 = +0.03
Î”wâ‚‚ = -0.1 Ã— (-0.3) Ã— 2 = +0.06
Î”wâ‚ƒ = -0.1 Ã— (-0.3) Ã— 3 = +0.09
```

Since we under-predicted (Å· < y), all weights **increase** to push the prediction higher next time! âœ“
